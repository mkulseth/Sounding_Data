{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; color: #086A87;\">Web Scrapping Python Script</h1>\n",
    "<h3 style=\"text-align: center; color: #086A87;\">By: McKenzie Kulseth</h3><br><br>\n",
    "<h4 style=\"color: #086A87;margin:0;\">Purpose of this code:</h4>\n",
    "<p style=\"color: #086A87; margin:0;\">This code will parse a webpage for data and extract that data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests as req\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable holding the name(s) of web-page(s)\n",
    "websites = [\"http://www.weathergraphics.com/identifiers/master-station.dat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!, There are 2779 stations in North America.\n"
     ]
    }
   ],
   "source": [
    "# Get the station ids\n",
    "station_ids = []\n",
    "for ws in websites:\n",
    "    source = req.get(ws).text\n",
    "    soup = BeautifulSoup(source,'lxml')\n",
    "    bod = soup.find('body')\n",
    "    text1 = bod.p.text\n",
    "    mytext1 = text1.split(\":\")\n",
    "    data1 = mytext1[-1].replace(\"_\",\"\")\n",
    "    data =  data1.replace(\"\\r\",\"\").split(\"\\n\")\n",
    "    headers = data[0].split(\" \")\n",
    "    for row in data[1:]:\n",
    "        try:\n",
    "            if row[0] == \"K\" and \"-9999\" not in row:\n",
    "                station_ids.append(row[1:4])\n",
    "        except:\n",
    "            continue\n",
    "print(\"Done!, There are \"+str(len(station_ids))+\" stations in North America.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating years, months, and days to loop through for getting data.\n"
     ]
    }
   ],
   "source": [
    "# Set variables to loop through dates\n",
    "today = date.today()\n",
    "this_year = int(today.strftime(\"%Y\"))\n",
    "years = np.arange(1973,this_year+1)\n",
    "months = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "\n",
    "days = ['01','02','03','04','05','06','07','08','09','10',\n",
    "          '11','12','13','14','15','16','17','18','19','20',\n",
    "          '21','22','23','24','25','26','27','28','29','30','31']\n",
    "print(\"Done creating years, months, and days to loop through for getting data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. There are 49621824 to sift through.\n"
     ]
    }
   ],
   "source": [
    "# Storing all of the websites in a list to be looped through to save sounding data.\n",
    "web_sites = []\n",
    "for y in years:\n",
    "    for m in months:\n",
    "        for d in days:\n",
    "            for ids in station_ids:\n",
    "                web_sites.append(\"http://weather.uwyo.edu/cgi-bin/sounding?region=naconf&TYPE=TEXT%3ALIST&YEAR=\"\n",
    "                                +str(y)+\"&MONTH=\"+str(m)+\"&FROM=\"\n",
    "                                +str(d)+\"00\"+\"&TO=\"+str(d)+\"12\"+\"&STNM=\"+ids[1:])\n",
    "print(\"Done. There are \"+str(len(web_sites))+\" to sift through.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LG\n",
      "55\n",
      "VD\n",
      "A0\n",
      "1A\n",
      "ET\n",
      "LX\n",
      "9J\n",
      "NB\n",
      "UO\n",
      "UB\n",
      "HM\n",
      "MX\n",
      "KL\n",
      "A1\n",
      "CU\n",
      "HN\n",
      "9J\n",
      "ZH\n",
      "J4\n",
      "A9\n",
      "OX\n",
      "AD\n",
      "KA\n",
      "DQ\n",
      "FM\n",
      "R1\n",
      "OB\n",
      "GM\n",
      "XF\n",
      "XX\n",
      "SL\n",
      "ZR\n",
      "EY\n",
      "AM\n",
      "R0\n",
      "UA\n",
      "XS\n",
      "EM\n",
      "OI\n",
      "CL\n",
      "VX\n",
      "00\n",
      "BT\n",
      "KA\n",
      "YH\n",
      "DH\n",
      "90\n",
      "EQ\n",
      "LD\n",
      "YV\n",
      "NA\n",
      "LP\n",
      "CU\n",
      "SM\n",
      "RO\n",
      "OT\n",
      "BR\n",
      "RF\n",
      "IT\n",
      "39\n",
      "EZ\n",
      "76\n",
      "LQ\n",
      "PK\n",
      "WT\n",
      "19\n",
      "ZK\n",
      "BF\n",
      "OG\n",
      "06\n",
      "UE\n",
      "RC\n",
      "LG\n",
      "RX\n",
      "SG\n",
      "GT\n",
      "XK\n",
      "RG\n",
      "WM\n",
      "01\n",
      "06\n",
      "FP\n",
      "HD\n",
      "ZC\n",
      "UG\n",
      "LG\n",
      "SX\n",
      "HU\n",
      "BN\n",
      "63\n",
      "EU\n",
      "YR\n",
      "CN\n",
      "V7\n",
      "V7\n",
      "GM\n",
      "HU\n",
      "65\n",
      "FZ\n",
      "3A\n",
      "LS\n",
      "GA\n",
      "E4\n",
      "JB\n",
      "VT\n",
      "UF\n",
      "HX\n",
      "RC\n",
      "74\n",
      "AD\n",
      "DL\n",
      "EZ\n",
      "03\n",
      "OW\n",
      "JN\n",
      "MX\n",
      "0G\n",
      "33\n",
      "34\n",
      "QE\n",
      "UM\n",
      "UX\n",
      "YL\n",
      "Y7\n",
      "11\n",
      "00\n",
      "AT\n",
      "MQ\n",
      "46\n",
      "PV\n",
      "CV\n",
      "UN\n",
      "VX\n",
      "NG\n",
      "AB\n",
      "BX\n",
      "UO\n",
      "YS\n",
      "35\n",
      "IH\n",
      "LU\n",
      "4Q\n",
      "LH\n",
      "2Q\n",
      "UR\n",
      "14\n",
      "XL\n",
      "YL\n",
      "SL\n",
      "ZZ\n",
      "RQ\n",
      "4Q\n",
      "IC\n",
      "CR\n",
      "EC\n",
      "JO\n",
      "RC\n",
      "CB\n",
      "AG\n",
      "L0\n",
      "2Q\n",
      "DW\n",
      "YX\n",
      "L2\n",
      "98\n",
      "JK\n",
      "ZJ\n",
      "KA\n",
      "HX\n",
      "UU\n",
      "0Q\n",
      "64\n",
      "AT\n",
      "CH\n",
      "UL\n",
      "GB\n",
      "RF\n",
      "18\n",
      "JO\n",
      "HR\n",
      "WD\n",
      "L7\n",
      "GT\n",
      "L9\n",
      "H1\n",
      "RS\n",
      "PL\n",
      "YK\n",
      "OC\n",
      "JF\n",
      "LC\n",
      "VK\n",
      "PC\n",
      "L8\n",
      "GB\n",
      "LI\n",
      "06\n",
      "QT\n",
      "AX\n",
      "HP\n",
      "AE\n",
      "55\n",
      "MH\n",
      "L6\n",
      "YV\n",
      "ER\n",
      "CE\n",
      "58\n",
      "OD\n",
      "HV\n",
      "HV\n",
      "O5\n",
      "IY\n",
      "SU\n",
      "RY\n",
      "HS\n",
      "WS\n",
      "PC\n",
      "ED\n",
      "A6\n",
      "L3\n",
      "KL\n",
      "FG\n",
      "34\n",
      "32\n",
      "KB\n",
      "KH\n",
      "VE\n",
      "XR\n",
      "79\n",
      "SP\n",
      "MD\n",
      "AO\n",
      "RB\n",
      "ES\n",
      "3Q\n",
      "3Q\n",
      "9Q\n",
      "96\n",
      "1Q\n",
      "4Q\n",
      "72\n",
      "13\n",
      "TD\n",
      "7Q\n",
      "5Q\n",
      "7Q\n",
      "NZ\n",
      "97\n",
      "9Q\n",
      "TV\n",
      "49\n",
      "39\n",
      "IU\n",
      "BL\n",
      "DD\n",
      "8Q\n",
      "IO\n",
      "WC\n",
      "IV\n",
      "AC\n",
      "HR\n",
      "CC\n",
      "MF\n",
      "AX\n",
      "NS\n",
      "8Q\n",
      "BD\n",
      "QL\n",
      "10\n",
      "UC\n",
      "UC\n",
      "DM\n",
      "KX\n",
      "GX\n",
      "OX\n",
      "EE\n",
      "AN\n",
      "YF\n",
      "ZY\n",
      "UX\n",
      "FO\n",
      "1Q\n",
      "NX\n",
      "JC\n",
      "HV\n",
      "BP\n",
      "SI\n",
      "DB\n",
      "NA\n",
      "44\n",
      "BA\n",
      "27\n",
      "XC\n",
      "5Q\n",
      "ZN\n",
      "MX\n",
      "MO\n",
      "L2\n",
      "ZP\n",
      "TS\n",
      "ZA\n",
      "21\n",
      "87\n",
      "L7\n",
      "VL\n",
      "6Q\n",
      "CK\n",
      "SU\n",
      "VE\n",
      "67\n",
      "82\n",
      "RM\n",
      "OA\n",
      "O6\n",
      "6Q\n",
      "RK\n",
      "XP\n",
      "KI\n",
      "45\n",
      "CB\n",
      "BG\n",
      "BX\n",
      "VW\n",
      "43\n",
      "CV\n",
      "IS\n",
      "VI\n",
      "54\n",
      "L3\n",
      "FF\n",
      "KO\n",
      "LS\n",
      "V9\n",
      "SE\n",
      "JC\n",
      "C0\n",
      "TR\n",
      "OS\n",
      "EF\n",
      "RV\n"
     ]
    }
   ],
   "source": [
    "# Get sounding data and save that data in files\n",
    "test_website_1=\"http://weather.uwyo.edu/cgi-bin/sounding?region=naconf&TYPE=TEXT%3ALIST&YEAR=2020&MONTH=11&FROM=0512&TO=0512&STNM=INL\"\n",
    "for test_website1 in web_sites:\n",
    "    test_website = test_website1\n",
    "    sounding_data = {\"PRES[hPa]\":[], \"HGHT[m]\":[], \"TEMP[C]\":[], \"DWPT[C]\":[], \"RELH[%]\":[],\n",
    "                     \"MIXR[g/kg]\":[], \"DRCT[deg]\":[], \"SKNT[knot]\":[], \"THTA[K]\":[],\n",
    "                     \"THTE[K]\":[], \"THTV[K]\":[]}\n",
    "    file_name = ''\n",
    "    mydf = pd.DataFrame()\n",
    "    headers = []\n",
    "    units = []\n",
    "    try:\n",
    "        source = req.get(test_website).text\n",
    "        soup = BeautifulSoup(source,\"html.parser\")\n",
    "        data = soup.find(\"pre\").text\n",
    "        data1 = data.split(\"\\n\")\n",
    "        if test_website[-3] != \"=\":\n",
    "            file_name = \"K\"+test_website[-3:]+\"_\"+test_website[-40:-36]+\"_\"+test_website[-29:-27]+\"_\"+test_website[-13:-11]+\"_sounding_data.csv\"\n",
    "        else:\n",
    "            file_name = \"K\"+test_website[-2:]+\"_\"+test_website[-39:-35]+\"_\"+test_website[-28:-26]+\"_\"+test_website[-12:-10]+\"_sounding_data.csv\"\n",
    "        headers = data1[2]\n",
    "        units = data1[3]\n",
    "        if headers != []:\n",
    "            for row in data1[5:]:\n",
    "                d1 = row[16:21].replace(\" \",\"\"); d2 = row[23:28].replace(\" \",\"\")\n",
    "                d3 = row[33:35].replace(\" \",\"\"); d4 = row[38:42].replace(\" \",\"\")\n",
    "                d5 = row[47:50].replace(\" \",\"\"); d6 = row[50:57].replace(\" \",\"\")\n",
    "                d7 = row[59:64].replace(\" \",\"\"); d8 = row[66:71].replace(\" \",\"\")\n",
    "                d9 = row[73:].replace(\" \",\"\"); d10 = row[1:7].replace(\" \",\"\")\n",
    "                d11 = row[9:14].replace(\" \",\"\")\n",
    "                if len(d1) != 0:\n",
    "                    sounding_data[\"TEMP[C]\"].append(float(d1))\n",
    "                elif len(d1) == 0:\n",
    "                    sounding_data[\"TEMP[C]\"].append(np.nan)\n",
    "                if len(d2) != 0:\n",
    "                    sounding_data[\"DWPT[C]\"].append(float(d2))\n",
    "                elif len(d2) == 0:\n",
    "                    sounding_data[\"DWPT[C]\"].append(np.nan)\n",
    "                if len(d3) != 0:\n",
    "                    sounding_data[\"RELH[%]\"].append(float(d3))\n",
    "                elif len(d3) == 0:\n",
    "                    sounding_data[\"RELH[%]\"].append(np.nan)\n",
    "                if len(d4) != 0:\n",
    "                    sounding_data[\"MIXR[g/kg]\"].append(float(d4))\n",
    "                elif len(d4) == 0:\n",
    "                    sounding_data[\"MIXR[g/kg]\"].append(np.nan)\n",
    "                if len(d5) != 0:\n",
    "                    sounding_data[\"DRCT[deg]\"].append(float(d5))\n",
    "                elif len(d5) == 0:\n",
    "                    sounding_data[\"DRCT[deg]\"].append(np.nan)\n",
    "                if len(d6) != 0:\n",
    "                    sounding_data[\"SKNT[knot]\"].append(float(d6))\n",
    "                elif len(d6) == 0:\n",
    "                    sounding_data[\"SKNT[knot]\"].append(np.nan)\n",
    "                if len(d7) != 0:\n",
    "                    sounding_data[\"THTA[K]\"].append(float(d7))\n",
    "                elif len(d7) == 0:\n",
    "                    sounding_data[\"THTA[K]\"].append(np.nan)\n",
    "                if len(d8) != 0:\n",
    "                    sounding_data[\"THTE[K]\"].append(float(d8))\n",
    "                elif len(d8) == 0:\n",
    "                    sounding_data[\"THTE[K]\"].append(np.nan)\n",
    "                if len(d9) != 0:\n",
    "                    sounding_data[\"THTV[K]\"].append(float(d9))\n",
    "                elif len(d9) == 0:\n",
    "                    sounding_data[\"THTV[K]\"].append(np.nan)\n",
    "                if len(d10) != 0:\n",
    "                    sounding_data[\"PRES[hPa]\"].append(float(d10))\n",
    "                elif len(d10) == 0:\n",
    "                    sounding_data[\"PRES[hPa]\"].append(np.nan)\n",
    "                if len(d11) != 0:\n",
    "                    sounding_data[\"HGHT[m]\"].append(float(d11))\n",
    "                elif len(d11) == 0:\n",
    "                    sounding_data[\"HGHT[m]\"].append(np.nan)\n",
    "        mydf = pd.DataFrame(sounding_data)\n",
    "        mydf.to_csv(file_name)\n",
    "    except:\n",
    "        if test_website[-3] == \"=\":\n",
    "            print(test_website[-2:])\n",
    "        else:\n",
    "            print(test_website[-3:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
