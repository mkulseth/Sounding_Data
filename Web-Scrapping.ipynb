{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; color: #086A87;\">Web Scrapping Python Script</h1>\n",
    "<h3 style=\"text-align: center; color: #086A87;\">By: McKenzie Kulseth</h3><br><br>\n",
    "<h4 style=\"color: #086A87;margin:0;\">Purpose of this code:</h4>\n",
    "<p style=\"color: #086A87; margin:0;\">This code will parse a webpage for data and extract that data. \n",
    "    <br><br><em>NOTE: One should orgainze the csv files being save into a directory tree to better organize the files.</p>\n",
    "<p style=\"color: #086A87;\"><br><strong>1.) Import Libraries</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: #086A87;margin: 0;\"> <br> <strong>2.) Get the station ids for the sounding stations from a website</strong>\n",
    "<ul style=\"color: #086A87;margin: 0;\"><li> Initialize list to ensure no station id is repeated in the list\n",
    "    <li> Loop through the website in the list of websites\n",
    "    <li> Use BeautifulSoup to parse the webpage for the station ids without the 'k' in front front\n",
    "    <li> Append those ids to the list of station ids</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = [\"http://www.weathergraphics.com/identifiers/master-station.dat\"]\n",
    "station_ids = []\n",
    "for ws in websites:\n",
    "    source = req.get(ws).text\n",
    "    soup = BeautifulSoup(source,'lxml')\n",
    "    bod = soup.find('body')\n",
    "    text1 = bod.p.text\n",
    "    mytext1 = text1.split(\":\")\n",
    "    data1 = mytext1[-1].replace(\"_\",\"\")\n",
    "    data =  data1.replace(\"\\r\",\"\").split(\"\\n\")\n",
    "    headers = data[0].split(\" \")\n",
    "    for row in data[1:]:\n",
    "        try:\n",
    "            if row[0] == \"K\" and \"-9999\" not in row:\n",
    "                station_ids.append(row[1:4])\n",
    "        except:\n",
    "            continue\n",
    "print(\"Done!, There are \"+str(len(station_ids))+\" stations in North America.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: #086A87;margin: 0;\"> <br><strong>3.) Creating lists to hold the dates to be looped through to find sounding data.</strong>\n",
    "<br> <ul style=\"color: #086A87;margin: 0;\"><li> Get today's date to get the ending year for sounding data to search for\n",
    "    <li> Create lists of years, months, and days to search for sounding data\n",
    "</ul></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "this_year = int(today.strftime(\"%Y\"))\n",
    "years = np.arange(1973,this_year+1)\n",
    "months = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "\n",
    "days = ['01','02','03','04','05','06','07','08','09','10',\n",
    "          '11','12','13','14','15','16','17','18','19','20',\n",
    "          '21','22','23','24','25','26','27','28','29','30','31']\n",
    "print(\"Done creating years, months, and days to loop through for getting data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: #086A87;\"><br><strong>4.) Loop through those dates and station ids to create a list of website to be parsed for sounding data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_sites = []\n",
    "for y in years:\n",
    "    for m in months:\n",
    "        for d in days:\n",
    "            for ids in station_ids:\n",
    "                web_sites.append(\"http://weather.uwyo.edu/cgi-bin/sounding?region=naconf&TYPE=TEXT%3ALIST&YEAR=\"\n",
    "                                +str(y)+\"&MONTH=\"+str(m)+\"&FROM=\"\n",
    "                                +str(d)+\"00\"+\"&TO=\"+str(d)+\"12\"+\"&STNM=\"+ids[1:])\n",
    "print(\"Done. There are \"+str(len(web_sites))+\" to sift through.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: #086A87;margin: 0;\"><br><strong> 5.) Get sounding data from websites</strong>\n",
    "<ul style=\"color: #086A87;margin: 0;\"><li> Loop through websites\n",
    "    <li> Store sounding data in dictionary\n",
    "    <li> Create a panadas dataframe to store the dictionary\n",
    "    <li> Save the dataframe as a csv file\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_ids_w_no_data = 0; websites_with_no_sounding_data = []\n",
    "websites_no_data = {}\n",
    "for ws in web_sites[0:3]:\n",
    "    sounding_data = {\"PRES[hPa]\":[], \"HGHT[m]\":[], \"TEMP[C]\":[], \"DWPT[C]\":[], \"RELH[%]\":[],\n",
    "                     \"MIXR[g/kg]\":[], \"DRCT[deg]\":[], \"SKNT[knot]\":[], \"THTA[K]\":[],\n",
    "                     \"THTE[K]\":[], \"THTV[K]\":[]}\n",
    "    file_name = ''\n",
    "    mydf = pd.DataFrame()\n",
    "    headers = []\n",
    "    units = []\n",
    "    try:\n",
    "        source = req.get(ws).text\n",
    "        soup = BeautifulSoup(source,\"html.parser\")\n",
    "        data = soup.find(\"pre\").text\n",
    "        data1 = data.split(\"\\n\")\n",
    "        if ws[-3] != \"=\":\n",
    "            file_name = \"K\"+ws[-3:]+\"_\"+ws[-40:-36]+\"_\"+ws[-29:-27]+\"_\"+ws[-13:-11]+\"_sounding_data.csv\"\n",
    "        else:\n",
    "            file_name = \"K\"+ws[-2:]+\"_\"+ws[-39:-35]+\"_\"+ws[-28:-26]+\"_\"+ws[-12:-10]+\"_sounding_data.csv\"\n",
    "        headers = data1[2]\n",
    "        units = data1[3]\n",
    "        if headers != []:\n",
    "            for row in data1[5:]:\n",
    "                d1 = row[16:21].replace(\" \",\"\"); d2 = row[23:28].replace(\" \",\"\")\n",
    "                d3 = row[33:35].replace(\" \",\"\"); d4 = row[38:42].replace(\" \",\"\")\n",
    "                d5 = row[47:50].replace(\" \",\"\"); d6 = row[50:57].replace(\" \",\"\")\n",
    "                d7 = row[59:64].replace(\" \",\"\"); d8 = row[66:71].replace(\" \",\"\")\n",
    "                d9 = row[73:].replace(\" \",\"\"); d10 = row[1:7].replace(\" \",\"\")\n",
    "                d11 = row[9:14].replace(\" \",\"\")\n",
    "                if len(d1) != 0:\n",
    "                    sounding_data[\"TEMP[C]\"].append(float(d1))\n",
    "                elif len(d1) == 0:\n",
    "                    sounding_data[\"TEMP[C]\"].append(np.nan)\n",
    "                if len(d2) != 0:\n",
    "                    sounding_data[\"DWPT[C]\"].append(float(d2))\n",
    "                elif len(d2) == 0:\n",
    "                    sounding_data[\"DWPT[C]\"].append(np.nan)\n",
    "                if len(d3) != 0:\n",
    "                    sounding_data[\"RELH[%]\"].append(float(d3))\n",
    "                elif len(d3) == 0:\n",
    "                    sounding_data[\"RELH[%]\"].append(np.nan)\n",
    "                if len(d4) != 0:\n",
    "                    sounding_data[\"MIXR[g/kg]\"].append(float(d4))\n",
    "                elif len(d4) == 0:\n",
    "                    sounding_data[\"MIXR[g/kg]\"].append(np.nan)\n",
    "                if len(d5) != 0:\n",
    "                    sounding_data[\"DRCT[deg]\"].append(float(d5))\n",
    "                elif len(d5) == 0:\n",
    "                    sounding_data[\"DRCT[deg]\"].append(np.nan)\n",
    "                if len(d6) != 0:\n",
    "                    sounding_data[\"SKNT[knot]\"].append(float(d6))\n",
    "                elif len(d6) == 0:\n",
    "                    sounding_data[\"SKNT[knot]\"].append(np.nan)\n",
    "                if len(d7) != 0:\n",
    "                    sounding_data[\"THTA[K]\"].append(float(d7))\n",
    "                elif len(d7) == 0:\n",
    "                    sounding_data[\"THTA[K]\"].append(np.nan)\n",
    "                if len(d8) != 0:\n",
    "                    sounding_data[\"THTE[K]\"].append(float(d8))\n",
    "                elif len(d8) == 0:\n",
    "                    sounding_data[\"THTE[K]\"].append(np.nan)\n",
    "                if len(d9) != 0:\n",
    "                    sounding_data[\"THTV[K]\"].append(float(d9))\n",
    "                elif len(d9) == 0:\n",
    "                    sounding_data[\"THTV[K]\"].append(np.nan)\n",
    "                if len(d10) != 0:\n",
    "                    sounding_data[\"PRES[hPa]\"].append(float(d10))\n",
    "                elif len(d10) == 0:\n",
    "                    sounding_data[\"PRES[hPa]\"].append(np.nan)\n",
    "                if len(d11) != 0:\n",
    "                    sounding_data[\"HGHT[m]\"].append(float(d11))\n",
    "                elif len(d11) == 0:\n",
    "                    sounding_data[\"HGHT[m]\"].append(np.nan)\n",
    "        mydf = pd.DataFrame(sounding_data)\n",
    "        mydf.to_csv(\"data/\"+file_name)\n",
    "    except:\n",
    "        if ws[-3] != \"=\":\n",
    "            if ws[-3] not in websites_with_no_sounding_data:\n",
    "                websites_with_no_sounding_data.append(ws[-3:])\n",
    "                websites_no_data.update({\"K\"+ws[-3:]:[]})                \n",
    "            websites_no_data[\"K\"+ws[-3:]].append(ws[-29:-27]+\"/\"+ws[-13:-11]+\"/\"+ws[-40:-36])\n",
    "            file_name = \"K\"+ws[-3:]+\"_\"+ws[-40:-36]+\"_\"+ws[-29:-27]+\"_\"+ws[-13:-11]+\"_sounding_data.csv\"\n",
    "        else:\n",
    "            if ws[-2] not in websites_with_no_sounding_data:\n",
    "                websites_with_no_sounding_data.append(ws[-2:])\n",
    "                websites_no_data.update({\"K\"+ws[-2:]:[]})                \n",
    "            websites_no_data[\"K\"+ws[-2:]].append(ws[-28:-26]+\"/\"+ws[-12:-10]+\"/\"+ws[-39:-35])\n",
    "            file_name = \"K\"+ws[-2:]+\"_\"+ws[-39:-35]+\"_\"+ws[-28:-26]+\"_\"+ws[-12:-10]+\"_sounding_data.csv\"\n",
    "        counting_ids_w_no_data += 1\n",
    "\n",
    "print(str(counting_ids_w_no_data)+\" of the \"+str(len(web_sites))+\" websites had no sounding information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: #086A87;\"><strong>6.) View the station ids and the dates they had know sounding data</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Station_ID\\tDates\")\n",
    "for key in websites_no_data.keys():\n",
    "    print(key+\"\\t\\t\",end=\"\")\n",
    "    for d in websites_no_data[key]:\n",
    "        if d != websites_no_data[key][-1]:\n",
    "            print(d,end=\", \")\n",
    "        else:\n",
    "            print(d+\".\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
